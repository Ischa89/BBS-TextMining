{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2a_opinion_lab_teamwork.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hgcEyn4DikNC"},"source":["# Opinion Mining and Sentiment Analysis: Teamwork\n","\n","**Text Mining unit**\n","\n","_Prof. Gianluca Moro, Dott. Ing. Nicola Piscaglia – DISI, University of Bologna_\n","\n","**Bologna Business School** - Alma Mater Studiorum Università di Bologna"]},{"cell_type":"markdown","metadata":{"id":"eeNMwbXnikNE"},"source":["## Instructions\n","- The provided exercises must be executed by teams of 2 or 3 persons indicated by the teacher, different teams should not communicate with each other\n","- It is allowed to consult course material and the Web for advice\n","- If still in doubt about anything, ask the teacher\n","\n","- At the end, the file must contain all the required results (as code cell outputs) along with all the commands necessary to reproduce them; \n","- the function of every command or group of related commands\n","must be documented clearly and concisely. \n","- The name of every variable defined in the commands (not counting the ones provided by the initial steps) must have the initial letters of your last names as a prefix (e.g. “sj_train_set” for Smith and Johnson). \n","- In order to work in pairs, you can access the same Google account that you created for your group and edit the notebook on Google Colab, but be careful to not overwrite the changes made by the other member of your group (to avoid this, you can edit a separate copy of the notebook and then merge the two members results before the end of the test). \n","- You have 1.5 hours to complete the test.\n","- When finished, the team member with an alphabetically lower surname will send the notebook file (having .ipynb extension) via mail (using the same Google Account as your group one) to the teachers (gianluca.moro@unibo.it; nicola.piscaglia@bbs.unibo.it) indicating “[BBS Teamwork] Your last names” as\n","subject, also keeping an own copy of the file for safety."]},{"cell_type":"markdown","metadata":{"id":"KDzrhbgXikNF"},"source":["## Setup\n","\n","The following cell contains all necessary imports"]},{"cell_type":"code","metadata":{"id":"Bby25YKDikNH"},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJQnNZqvikNM"},"source":["Run the following to download the necessary files"]},{"cell_type":"code","metadata":{"id":"YpETftXZikNN"},"source":["import os\n","from urllib.request import urlretrieve\n","def download(file, url):\n","    if not os.path.exists(file):\n","        urlretrieve(url, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6cZh6mEikNR"},"source":["download(\"100k_reviews.tsv.gz\", \"https://www.dropbox.com/s/9fkjz84dnzfyimt/estore_reviews_100k.tsv.gz?dl=1\")\n","download(\"positive-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/gh-pages/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/positive-words.txt\")\n","download(\"negative-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/gh-pages/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhTusR7VikNV"},"source":["nltk.download(\"punkt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wn2QTmSEikNd"},"source":["## Dataset\n","\n","We provide in the `100k_reviews.tsv.gz` file a dataset of 100,000 reviews posted on Amazon.com about DVDs of movies and TV series. Each review is labeled with a score between 1 and 5 stars.\n","\n","Run the following to correctly load the file into a pandas DataFrame."]},{"cell_type":"code","metadata":{"id":"wxNeTVsLikNe"},"source":["data = pd.read_csv(\"100k_reviews.tsv.gz\", sep=\"\\t\", compression=\"gzip\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFUduq4IikNk"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xeomWAf6ikNp"},"source":["Within the teamwork you will also make use of the Hu and Liu sentiment lexicon: run the following to load sets of positive and negative words."]},{"cell_type":"code","metadata":{"id":"uNKZ9MTzikNq"},"source":["def scan_hu_liu(f):\n","    for line in f:\n","        line = line.decode(errors=\"ignore\").strip()\n","        if line and not line.startswith(\";\"):\n","            yield line\n","\n","def load_hu_liu(filename):\n","    with open(filename, \"rb\") as f:\n","        return set(scan_hu_liu(f))\n","\n","hu_liu_pos = load_hu_liu(\"positive-words.txt\")\n","hu_liu_neg = load_hu_liu(\"negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOMrk7rcikNu"},"source":["## Exercises"]},{"cell_type":"markdown","metadata":{"id":"OgjLy-C3ikNv"},"source":["**1)** Verify the distribution of the number of stars"]},{"cell_type":"markdown","metadata":{"id":"LugRAMAQikNv"},"source":["**2)** Add a `label` column to the DataFrame whose value is `\"pos\"` for reviews with 4 or 5 stars and `\"neg\"` for reviews with 3 stars or less"]},{"cell_type":"markdown","metadata":{"id":"w_L1dmE0ikNw"},"source":["**3)** Split the dataset randomly into a training set with 80\\% of data and a test set with the remaining 20\\%"]},{"cell_type":"markdown","metadata":{"id":"flvcVGNSikNw"},"source":["**4)** Create a function which accepts a text as input, counts the occurrences of positive and negative words from the Hu \\& Liu lexicon and return `\"pos\"` if there are more positive words than negative or `\"neg\"` otherwise"]},{"cell_type":"markdown","metadata":{"id":"f_cEvO19ikNx"},"source":["**4)** Apply the function above to test reviews and get the percentage of cases where the function output matches the known label"]},{"cell_type":"markdown","metadata":{"id":"3kQeqCHpikNy"},"source":["**5)** Create a tf.idf vector space model from training reviews excluding words appearing in less than 3 documents and extract the document-term matrix for them"]},{"cell_type":"markdown","metadata":{"id":"FcyHoXVRikNy"},"source":["**6)** Train a logistic regression classifier on the training reviews, using the representation created above"]},{"cell_type":"markdown","metadata":{"id":"pqs1tT5jikNz"},"source":["**7)** Verify the accuracy of the classifier on the test set"]},{"cell_type":"markdown","metadata":{"id":"UwlMxG83ikN0"},"source":["**8)** Extract the 10 words with the highest regression coefficient and the 10 words with the lowest coefficient"]},{"cell_type":"markdown","metadata":{"id":"_SKd8u7TikN0"},"source":["**9)** Create a function which accepts a text as input and returns a list of the only words from the text which are also present in the Hu and Liu lexicon (each distinct word must appear in the list as many times as it appears in the text)"]},{"cell_type":"markdown","metadata":{"id":"Kj8cuPsMikN1"},"source":["**10)** Repeat points from 5 to 7 with a tf.idf vectorizer which uses the function above to extract tokens from text"]}]}