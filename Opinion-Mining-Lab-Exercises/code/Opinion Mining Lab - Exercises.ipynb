{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Opinion Mining Lab - Exercises.ipynb","provenance":[],"collapsed_sections":["wlq0hdJOuWtn","oG8M1yTUX65e","wvsmOXnZY82_"],"authorship_tag":"ABX9TyNDvTQHtAI6TgTpGGtVxMx5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fzuf3cVHi6TS"},"source":["# Opinion Mining and Sentiment Analysis - Exercises\n","\n","**Text Mining unit**\n","\n","_Prof. Gianluca Moro, Dott. Ing. Nicola Piscaglia – DISI, University of Bologna_\n","\n","**Bologna Business School** - Alma Mater Studiorum Università di Bologna"]},{"cell_type":"markdown","metadata":{"id":"C9sHk878uM5U"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"wlq0hdJOuWtn"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"MVjtrtsuD3te"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","from statsmodels.stats.contingency_tables import mcnemar\n","import os\n","from urllib.request import urlretrieve\n","import glob\n","import gzip\n","import json\n","import nltk\n","nltk.download(\"punkt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oG8M1yTUX65e"},"source":["### Download utility function"]},{"cell_type":"code","metadata":{"id":"7JL5tJHlX4jr"},"source":["# Download a file from an URL\n","def download(file, url):\n","    if not os.path.isfile(file):\n","        urlretrieve(url, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wnkMJMkxYA7F"},"source":["### Datasets Downloading"]},{"cell_type":"code","metadata":{"id":"IjkqeJGrrdH_"},"source":["# Dataset filenames\n","DICT_DATASET = \"reviews_Clothing_Shoes_and_Jewelry.json.gz\"\n","\n","# Dataset downloading\n","download(DICT_DATASET, \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\")\n","download('All_Beauty.json.gz','http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Beauty.json.gz')\n","download(\"positive-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/3ad6643b698f652f200dfbf463a3cb49de8c0e9f/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/positive-words.txt\")\n","download(\"negative-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/3ad6643b698f652f200dfbf463a3cb49de8c0e9f/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YAjmHi5YHrG"},"source":["# Check if the files have been successfully downloaded\n","print(glob.glob(\"*\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OpDNZWvi6T7"},"source":["Within the exercises you will also make use of the Hu and Liu sentiment lexicon: run the following to load sets of positive and negative words."]},{"cell_type":"code","metadata":{"id":"WbTxjuJBi6T8"},"source":["def scan_hu_liu(f):\n","    for line in f:\n","        line = line.decode(errors=\"ignore\").strip()\n","        if line and not line.startswith(\";\"):\n","            yield line\n","\n","def load_hu_liu(filename):\n","    with open(filename, \"rb\") as f:\n","        return set(scan_hu_liu(f))\n","\n","hu_liu_pos = load_hu_liu(\"positive-words.txt\")\n","hu_liu_neg = load_hu_liu(\"negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvsmOXnZY82_"},"source":["### Dataset loading"]},{"cell_type":"code","metadata":{"id":"AaNkd8AF9gya"},"source":["total_samples = 5000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8x6wt8OZAAj"},"source":["def load_dataset(dataset_path):\n","    # We do not consider 3-stars rating as they can be confuse the model in\n","    # opinion polarity discrimination\n","    pos_overall_values = {5.0, 4.0}\n","    neg_overall_values = {1.0, 2.0}\n","    \n","    # Data structures\n","    data = []\n","    overall = []\n","    \n","    # Data loading: read all the dataset\n","    print(\"Loading json file...\")\n","    \n","    # Reading dataset: we build our dataset by selecting\n","    # only the reviews which are not 3 stars rated. For each review added to our\n","    # dataset we add to \"overall\" the label for that review.\n","    with gzip.open(dataset_path) as jsonfile:\n","        index = 0\n","        pos = 0\n","        neg = 0\n","        # Each line in the json file represents a review\n","        for line in jsonfile:\n","            review = json.loads(line)\n","            # \n","            if review['overall'] in pos_overall_values and pos < int(total_samples / 2):\n","                index += 1\n","                # We keep track of the number of positive reviews read\n","                pos += 1\n","                # Review appending to our dataset\n","                data.append(review)\n","                # Label appending\n","                overall.append(1)\n","                if index >= total_samples:\n","                    break\n","            elif review['overall'] in neg_overall_values and neg < int(total_samples / 2):\n","                index += 1\n","                # We keep track of the number of negative reviews read\n","                neg += 1\n","                data.append(review)\n","                # Label appending\n","                overall.append(0)\n","                # We stop reading if we reached the maximum number of samples\n","                if index >= total_samples:\n","                    break\n","    \n","    # Select only the review text for each review object\n","    reviewtext = [value['reviewText'] for value in data]\n","    \n","    return reviewtext, overall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qif9CmIw_Jce"},"source":["texts, labels = load_dataset(DICT_DATASET)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yN8F4SNVuh3x"},"source":["### Exercises"]},{"cell_type":"markdown","metadata":{"id":"M8b4P0WG-YXh"},"source":["**1)** Create a pandas Dataframe containing the provided data (texts and labels). "]},{"cell_type":"markdown","metadata":{"id":"RZUDfp1kC3Ea"},"source":["**2)** Then, randomly split the loaded dataset into training and test set\n","* using an hold-out approach (e.g. the training set is composed by the 90% of the dataset reviews) \n","* and keeping the dataset cardinality balanced by label. "]},{"cell_type":"markdown","metadata":{"id":"s2ZiEDjbDBCv"},"source":["**3)** Afterwards, verify the labels distribution over the training and test sets. "]},{"cell_type":"markdown","metadata":{"id":"P64mhRbX94aT"},"source":["**4)** A lexicon with sets of commonly used positive and negative words is provided in the variables pos_words\n","and neg_words, respectively. \n","\n","Classify the reviews in the test set by first assigning to\n","each a score equal to the number of known positive words within it minus the number of negative words,\n","then return `1` for reviews with a positive score and `0` for reviews with a negative or null score.\n","\n","To calculate the score value for each word: sum `1` or `-1` for each positive/negative word respectively and sum `2` or `-2` for each positive/negative word respectively that is preceded by the word \"very\".\n","\n","Finally, evaluate the classification accuracy, i.e. the ratio between the number of correctly classified reviews and\n","the total count of test reviews."]},{"cell_type":"markdown","metadata":{"id":"BLD_buBZi6U6"},"source":["**5)** Create a tf.idf vector space model from training reviews excluding words appearing in less than 5 documents and extract the document-term matrix for them"]},{"cell_type":"markdown","metadata":{"id":"F7Xy6Oymi6VC"},"source":["**6)** Train a Bernoulli Naive Bayes classifier on the training reviews, using the representation created above"]},{"cell_type":"markdown","metadata":{"id":"K1gSb6gDi6VM"},"source":["**7)** Verify the accuracy of the classifier on the test set"]},{"cell_type":"markdown","metadata":{"id":"Q_uXuv2QHXm3"},"source":["**8)** Repeat steps from 5 to 7, this time using a TfIdf Vectorizer and a Multinomial NB model"]},{"cell_type":"markdown","metadata":{"id":"EQBGfhA-IkNd"},"source":["**9)** Repeat steps from 5 to 7, this time using TF-IDF Vectorizer that includes also bigrams and trigrams as features and a Multinomial NB model as classifier."]},{"cell_type":"markdown","metadata":{"id":"Ms4WpgV_KJMO"},"source":["**10)** Complete the followings tasks:\n","\n","1. Use the code cell below to load a new dataset in `beauty_df` containing Amazon review on beauty products. \n","\n","2. Then, create a new pandas dataframe named `beauty_data` selecting only `reviewText`, `overall` columns.\n","\n","3. Add a `label` column to the DataFrame whose value is `1` for reviews with 4 or 5 stars and `0` for reviews with 3 stars or less\n","\n","4. Test the previous Multinomial NB model (used in point **9**) calculating the mean accuracy on beauty reviews contained in `beauty_data`"]},{"cell_type":"code","metadata":{"id":"mRnNzelIMreh"},"source":["def parse(path):\n","  g = gzip.open(path, 'r')\n","  for l in g:\n","    yield json.loads(l)\n","\n","def getDF(path):\n","  i = 0\n","  df = {}\n","  for d in parse(path):\n","    df[i] = d\n","    i += 1\n","  return pd.DataFrame.from_dict(df, orient='index').sample(10000)\n","\n","# Loading data into a pandas dataframe\n","beauty_df = getDF('All_Beauty.json.gz')\n","beauty_df[\"reviewText\"] = beauty_df[\"reviewText\"].apply(lambda x: np.str_(x)) # encoding the strings as unicode ones\n","beauty_df.head() # print first 5 entries"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64op5SqHKKbt"},"source":["**11)** Perform the following tasks:\n","1. Get the prediction lists on `beauty_data` by the latest MultinomialNB model and the first unsupervised model (based on opinion words list)\n","2. Finally, compare the two models by performing a Mcnemar test using the provided `mcnemar_pvalue` function below. You must call the function passing the two model prediction lists as parameters (e.g. `mcnemar_pvalue(preds1, preds2)`). \n","  * This function returns the pvalue of the test where the null hypothesis is that the two models have the same error proportions\n","  * The pvalue here represent the probability that the difference between the proportions of the compared models errors is obtained by chance, in other words the probability the models have the same proportion of errors. Thus, the greater the pvalue the more similar the two models. \n","  * e.g. The p-value of 0.000 signifies that the difference between the two proportions of errors is statistically significant.\n","3. Decide if the two models are similar\n","  * Set a confidence level of 0.95\n","  * Check if `p-value > (1 - confidence level) `\n","  "]},{"cell_type":"code","metadata":{"id":"brPNp3uSldQC"},"source":["def mcnemar_pvalue(model1_predictions, model2_predictions):\n","    # define contingency table\n","    table = pd.crosstab(model1_predictions, model2_predictions)\n","    print(table)\n","\n","    # calculate mcnemar test\n","    result = mcnemar(table)\n","    return result.pvalue"],"execution_count":null,"outputs":[]}]}